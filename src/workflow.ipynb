{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU llama-index llama-index-utils-workflow\n",
    "%pip install -qU pinecone llama-index-vector-stores-pinecone\n",
    "%pip install -qU llama-index-embeddings-mistralai llama-index-llms-text-generation-inference\n",
    "%pip install -qU llama-index-core llama-parse llama-index-readers-file python-dotenv\n",
    "%pip install -qU llama-index-readers-file\n",
    "%pip install -qU \"openinference-instrumentation-llama-index>=3.0.0\" \"opentelemetry-proto>=1.12.0\" opentelemetry-exporter-otlp opentelemetry-sdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API = os.environ[\"PINECONE_API\"]\n",
    "PINECONE_INDEX_NAME = os.environ[\"PINECONE_INDEX_NAME\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "PHOENIX_API_KEY = os.environ[\"PHOENIX_API_KEY\"]\n",
    "OTEL_EXPORTER_OTLP_HEADERS = os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"]\n",
    "PHOENIX_CLIENT_HEADERS = os.environ[\"PHOENIX_CLIENT_HEADERS\"]\n",
    "PHOENIX_COLLECTOR_ENDPOINT = os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://app.phoenix.arize.com'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PHOENIX_COLLECTOR_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christos/AIM/ragathon24/mlops-club-rag/.venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API\"])\n",
    "index_name = os.environ[\"PINECONE_INDEX_NAME\"]\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "# embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "# Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "DEFAULT_RAG_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"Use the provided context to answer the question. If you don't know the answer, say you don't know.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class PrepEvent(Event):\n",
    "    \"\"\"Prep event (prepares for retrieval).\"\"\"\n",
    "    pass\n",
    "\n",
    "class RetrieveEvent(Event):\n",
    "    \"\"\"Retrieve event (gets retrieved nodes).\"\"\"\n",
    "\n",
    "    retrieved_nodes: list[NodeWithScore]\n",
    "\n",
    "class AugmentGenerateEvent(Event):\n",
    "    \"\"\"Query event. Queries given relevant text and search text.\"\"\"\n",
    "    relevant_text: str\n",
    "    search_text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    ")\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "\n",
    "# First things first, we need to create a new class that subclasses Workflow.\n",
    "# Each step, now, is a method (decorated by the @step decorator) which will take an Event and Context as input.\n",
    "class WorkflowRAG(Workflow):\n",
    "    @step\n",
    "    async def initialize_index(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Initializing index.\"\"\"\n",
    "\n",
    "        pc = Pinecone(api_key=os.environ[\"PINECONE_API\"])\n",
    "        index_name = os.environ[\"PINECONE_INDEX_NAME\"]\n",
    "        pinecone_index = pc.Index(index_name)\n",
    "\n",
    "        vector_store = PineconeVectorStore(\n",
    "            pinecone_index=pinecone_index,\n",
    "            add_sparse_vector=True,\n",
    "        )\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            [], storage_context=storage_context\n",
    "        )\n",
    "        return StopEvent(result=index)\n",
    "\n",
    "    @step\n",
    "    async def prepare_for_retrieval(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> PrepEvent | None:\n",
    "        \"\"\"Prepare for retrieval.\"\"\"\n",
    "\n",
    "        query_str: str | None = ev.get(\"query_str\")\n",
    "        retriever_kwargs: dict | None = ev.get(\"retriever_kwargs\", {})\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        llm=OpenAI(model=\"gpt-4o-mini\")\n",
    "        await ctx.set(\"rag_pipeline\", QueryPipeline(\n",
    "            chain=[DEFAULT_RAG_PROMPT, llm]\n",
    "        ))\n",
    "\n",
    "        await ctx.set(\"llm\", llm)\n",
    "        await ctx.set(\"index\", index)\n",
    "\n",
    "        await ctx.set(\"query_str\", query_str)\n",
    "        await ctx.set(\"retriever_kwargs\", retriever_kwargs)\n",
    "\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: PrepEvent\n",
    "    ) -> RetrieveEvent | None:\n",
    "        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "        retriever_kwargs = await ctx.get(\"retriever_kwargs\")\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        index = await ctx.get(\"index\", default=None)\n",
    "        if not (index):\n",
    "            raise ValueError(\n",
    "                \"Index and tavily tool must be constructed. Run with 'documents' and 'tavily_ai_apikey' params first.\"\n",
    "            )\n",
    "\n",
    "        retriever: BaseRetriever = index.as_retriever(\n",
    "            **retriever_kwargs\n",
    "        )\n",
    "        result = retriever.retrieve(query_str)\n",
    "        await ctx.set(\"query_str\", query_str)\n",
    "        return RetrieveEvent(retrieved_nodes=result)\n",
    "\n",
    "    @step\n",
    "    async def augment_and_generate(self, ctx: Context, ev: RetrieveEvent) -> StopEvent:\n",
    "        \"\"\"Get result with relevant text.\"\"\"\n",
    "        relevant_nodes = ev.retrieved_nodes\n",
    "        relevant_text = \"\\n\".join([node.get_content() for node in relevant_nodes])\n",
    "        query_str = await ctx.get(\"query_str\")\n",
    "\n",
    "        relevancy_pipeline = await ctx.get(\"rag_pipeline\")\n",
    "\n",
    "        relevancy = relevancy_pipeline.run(\n",
    "                context=relevant_text, question=query_str\n",
    "        )\n",
    "\n",
    "        return StopEvent(result=relevancy.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wf_rag_workflow.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    WorkflowRAG, filename=\"wf_rag_workflow.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_workflow = WorkflowRAG()\n",
    "index = await rag_workflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To install Oh My Zsh, you first need to have Zsh installed on your machine. Here are the steps to install Oh My Zsh:\n",
       "\n",
       "1. **Install Zsh**: If you don't have Zsh installed, you can do so using the following commands:\n",
       "   - On Mac: `brew install zsh`\n",
       "   - On Ubuntu: `sudo apt-get install zsh`\n",
       "\n",
       "2. **Install Oh My Zsh**: Once Zsh is installed, you can install Oh My Zsh using one of the following commands:\n",
       "   - Using `curl`: \n",
       "     ```bash\n",
       "     sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n",
       "     ```\n",
       "   - Using `wget`: \n",
       "     ```bash\n",
       "     sh -c \"$(wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\"\n",
       "     ```\n",
       "\n",
       "3. **Follow the prompts**: After running the installation command, you may need to enter your password and follow any additional prompts.\n",
       "\n",
       "Make sure to check the official Oh My Zsh website for the most up-to-date installation instructions and any additional options."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "response = await rag_workflow.run(\n",
    "    query_str=\"How do I install oh my zsh?\",\n",
    "    index=index,\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
